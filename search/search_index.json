{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with LULU Supercomputers (HPC)","text":"<p>Supercomputers offer immense computing power for demanding tasks, but using them requires some understanding. This guide breaks down the key steps:</p>"},{"location":"#1-accessing-hpc","title":"1. Accessing HPC","text":"<ul> <li> <p>Registration or Proposal Submission: Depending on the specific supercomputer, register for a user account or submit a project proposal for computing resources.</p> </li> <li> <p>Logging In: Utilizes SSH to access login nodes, typically reserved for users.</p> </li> <li> <p>SSH (Secure Shell) keys: Enhances the authentication and security of your SSH connection.</p> </li> <li> <p>Moving Data: Common tools include <code>rsync</code> for mirroring directories, <code>scp</code> for transferring individual files or folders.</p> </li> </ul>"},{"location":"#2-running-jobs","title":"2. Running Jobs","text":"<ul> <li> <p>Batch Schedulers: HPC systems use schedulers to allocate resources for user jobs. These jobs are typically defined by a script outlining commands and the necessary resources such as time, memory, and processors.</p> </li> <li> <p>Job Scripts: Write a job script specifying the tasks (commands) and the resources your application needs. Supercomputer documentation or support resources can help with this.</p> </li> <li> <p>Submission and Execution: Submit your job script to the scheduler. It waits for available resources and then runs your job on the allocated hardware. This may involve a delay depending on other users' jobs.</p> </li> </ul> <p>Warning</p> <p>Oops! Lulu's system will update sections 3 and 4 soon.</p>"},{"location":"#3-modules","title":"3. Modules","text":"<ul> <li> <p>Managing Software: The Module System allows loading specific software packages (compilers, libraries) without manual installation. </p> </li> <li> <p>Module Commands: Use commands like <code>module list</code>, <code>module avail</code>, <code>module load/unload</code>, and <code>module switch</code> to interact with the Module System.</p> </li> </ul>"},{"location":"#4-parallel-programming","title":"4. Parallel Programming","text":"<ul> <li>Beyond Single Cores: Modern HPCrely on parallel programming techniques like OpenMP and MPI to distribute tasks across multiple processors, significantly accelerating execution.</li> </ul>"},{"location":"connecting/copying-data/","title":"File Transfer","text":"<p>Moving files/folders to or from the HPC offers various methods. </p>"},{"location":"connecting/copying-data/#secure-copy-scp","title":"Secure Copy <code>scp</code>","text":"<p>For single file/folder transfers, <code>scp</code> through ssh is convenient and widely compatible. Here's a basic usage:</p> <p><pre><code>scp [from] [to]\n</code></pre> The source (<code>from</code>) and destination (<code>to</code>) can each be a filename or a directory on your local machine or a remote host.</p> <p>Upload a File from Your Computer to a Cluster</p> <pre><code>$ scp foobar.txt your_username@clustername:/path/to/remote/directory\n</code></pre> <p>In this scenario, <code>foobar.txt</code> is copied to the directory <code>/path/to/remote/directory</code> on remote machine. If <code>foobar.txt</code> is not in your current directory, you can specify the full path:</p> <p>Upload a Directory to a Cluster</p> <pre><code>$ scp -r your_directory your_username@clustername:/path/to/remote/directory\n</code></pre> <p>Download a File from the Cluster to Your Computer</p> <pre><code>$ scp your_username@clustername:/path/to/remote/foobar.txt .\n</code></pre> <p>Here, <code>.</code> represents your current working directory. To specify a different destination, replace <code>.</code> with the full path:</p> <p><pre><code>$ scp your_username@clustername:/path/to/remote/foobar.txt /path/to/local/your_folder\n</code></pre> Further documentation on <code>scp</code> can be found here.</p>"},{"location":"connecting/copying-data/#rsync","title":"<code>rsync</code>","text":"<p>For more complex transfers, <code>rsync</code> offers efficiency, especially for multiple files or entire directory trees. Here's a basic command:</p> <pre><code>$ rsync [options] [source] [destination]\n</code></pre> <pre><code># Connect to the transfer node from the login node\n$ ssh transfer\n\n# Copy a Directory from Local to Remote Server\n$ rsync -avzh path/to/local your_username@clustername:/path/to/remote/directory\n\n# Copy a Directory from Remote to Local Server\n$ rsync -avzh your_username@clustername:/path/to/remote/directory path/to/local\n</code></pre> <p>The <code>-azvh</code> options enable compressed transfer with readable output, preserving timestamps. Another common option, `-r``, synchronizes all subdirectories recursively.</p> <p>Further <code>rsync</code> documentation is available here.</p>"},{"location":"connecting/copying-data/#rclone","title":"Rclone","text":"<p>Use Rclone for cloud storage (Box, Dropbox, Wasabi, AWS S3, or Google Cloud Storage, etc.) transfers. Configure Rclone for each storage type and protect configurations with passwords.</p> <pre><code># Configure Rclone for each storage type using\n$ rclone configure\n</code></pre> <p>During configuration, assign a unique name (e.g., <code>mys3</code>) and provide connection details. After saving, connect to the transfer node (via <code>ssh transfer</code> from the login node) and use the assigned name for file transfers.</p> <pre><code># Copy files to cloud storage\n$ rclone copy localpath/myfile mys3:bucketname/\n\n$ rclone sync localpath/mydir mys3:bucketname/remotedir\n</code></pre> <p>An example of executing rclone can be found here.</p>"},{"location":"connecting/copying-data/#file-transfer-protocol-ftp","title":"File Transfer Protocol (ftp)","text":"<p>FTP, utilized with programs like Filezilla, WinSCP (Windows) or Cyberduck  (OS/Windows), is a network protocol for file exchange with a server. Utilizing such programs with graphical interfaces offers beginners flexibility and ease of use.</p>"},{"location":"connecting/generate-key/","title":"SSH (Secure Shell) keys","text":"<p>When connecting via SSH, entering your password every time can be repetitive. SSH keys offer a password-free alternative, enhancing both convenience and security. </p> <p>Warning</p> <p>SMILE's cluster still uses passwords, while LULU's cluster requires users to submit public keys for authentication.</p> <p>Info</p> <p>Currently, GitHub utilizes SSH for accessing and editing data in its repositories.</p>"},{"location":"connecting/generate-key/#how-do-ssh-keys-work","title":"How do SSH-Keys work?","text":"<p>Imagine having a two-part security system: a public key (like a lock) and a private key (like a key). Anyone can see the public key, but only your private key unlocks it. Similarly, your public key is shared with the server, while your private key stays secret.</p> <p>Traditionally, the RSA algorithm was used for SSH keys. However, elliptic curve cryptography (specifically, <code>ed25519</code>) has emerged as a more secure and efficient alternative. We recommend using <code>ed25519</code> SSH keys for enhanced security and performance.</p> <p>For deeper understanding, explore these Wikipedia articles about public-key cryptography and challenge-response authentication.</p>"},{"location":"connecting/generate-key/#setting-up-ssh-keys-macoslinux","title":"Setting Up SSH Keys (macOS/Linux)","text":""},{"location":"connecting/generate-key/#1-generate-a-key-pair","title":"1. Generate a key pair:","text":"<p>Check if your SSH already has a key by verifying if a file named <code>~/.ssh/id_xxx.pub</code> exists. If not, create a key using the following command (including your email address for easy identification later on):</p> <pre><code>$ ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> <p>Your terminal should respond:</p> <pre><code>Generating public/private ed25519 key pair.\n</code></pre> <p>Do not change the filename. Press \u201cEnter\u201d. Choose a strong passphrase to protect your account if your private key is compromised.</p> <pre><code>Enter passphrase (empty for no passphrase):\nEnter same passphrase again:\n</code></pre> <p>The key pair is saved in the <code>.ssh</code> directory in your home folder, with the public key located at <code>~/.ssh/id_ed25519.pub</code>. If you forget your passphrase, recovery isn't possible; you'll have to generate and upload a new SSH key pair.</p> <p>The session should resemble the following:</p> <pre><code>host:~$ ssh-keygen -t ed25519 -C \"your_email@example.com\"\nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/home/USER/.ssh/id_ed25519): \nEnter passphrase (empty for no passphrase):\nEnter same passphrase again: \nYour identification has been saved in /home/USER/.ssh/id_ed25519.\nYour public key has been saved in /home/USER/.ssh/id_ed25519.pub.\nThe key fingerprint is:\nSHA256:Z6InW1OYt3loU7z14Kmgy87iIuYNr1gJAN1tG71D7Jc your_email@example.com\nThe key's randomart image is:\n+--[ED25519 256]--+\n|.. . . o         |\n|. . . + +        |\n|.    . = . .     |\n|.     . +oE.     |\n|.       So= o o  |\n| . .   . * = + + |\n|  +   o + B o o .|\n| oo+. .B + + .   |\n|.ooooooo*.  .    |\n+----[SHA256]-----+\n</code></pre> <p>The content of ~/.ssh/id_ed25519.pub should resemble this:</p> <pre><code>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFzuiaSVD2j5y6RlFxOfREB/Vbd+47ABlxF7du5160ZH your_email@example.com\n</code></pre> <p>To avoid entering the passphrase each time, you can add your key to the SSH agent, which manages keys and remembers passphrases.</p> <pre><code>$ eval \"$(ssh-agent -s)\"\n$ ssh-add\n</code></pre> <p>For MacOS connection issues with SSH keys, try:</p> <pre><code>$ ssh-add --apple-use-keychain\n</code></pre> <p>To verify that the agent is running and your key is loaded, use:</p> <pre><code>$ ssh-add -l\n</code></pre> <p>Ensure the command prints at least one key, displaying the key size, fingerprint hash, and file location in the file system.</p>"},{"location":"connecting/generate-key/#2-upload-your-public-key","title":"2. Upload your public key:","text":"<p>Copy the output of:</p> <p><pre><code>$ cat ~/.ssh/id_ed25519.pub\n</code></pre> and share it with the cluster administrator.</p>"},{"location":"connecting/generate-key/#3-connect-to-a-cluster","title":"3. Connect to a cluster","text":"<pre><code>$ ssh your_username@clustername\n</code></pre>"},{"location":"connecting/generate-key/#using-windows","title":"Using Windows","text":"<p>We recommend using MobaXterm on Windows for SSH connections as it simplifies SSH key management. </p> <p>To get MobaXterm:</p> <ul> <li> <p>Visit https://mobaxterm.mobatek.net/download-home-edition.html</p> </li> <li> <p>Download either the Portable edition (blue button on the left, suitable for users without admin rights) or the Installer edition (green button on the right, requires admin rights).</p> </li> <li> <p>Install or unpack MobaXterm and launch the software. Remember to dismiss any firewall warnings that may appear.</p> </li> </ul> <p>Here's how to set up SSH keys with MobaXterm:</p> <ul> <li> <p>Open MobaXterm</p> </li> <li> <p>Go to Tools -&gt; MobaKeyGen (SSH key generator)</p> </li> <li> <p>Click \"Generate\" with default settings</p> </li> <li> <p>Move your mouse to generate randomness</p> </li> <li> <p>Save the public key as id_rsa.pub and the private key (keep this secret!) as id_rsa.ppk</p> </li> <li> <p>Upload your public key: Copy the public key text and forward it to the cluster administrator</p> </li> <li> <p>Connect to a cluster using MobaXterm</p> </li> </ul>"},{"location":"connecting/generate-key/#connecting-from-another-computerlaptop","title":"Connecting from Another Computer/Laptop","text":"<p>To connect to the cluster from a different computer than the one where your SSH keys are stored, you have two options:</p> <ul> <li> <p>Generate a new SSH key pair and submit the public part as explained earlier.</p> </li> <li> <p>Copy the private part of the SSH key (<code>~/.ssh/id_ed25519</code>) to the second computer in the same location.</p> </li> </ul> <pre><code>$ cd ~/.ssh\n$ chmod g-rwx id_ed25519*\n$ ssh-add id_ed25519\n</code></pre> <p>Danger</p> <p>Do not keep the key on any USB stick. Delete it after transferring the file. This is sensitive data. Ensure that the files are only readable by you.</p>"},{"location":"connecting/getting-account/","title":"Getting an account","text":"<p>To gain access to High-Performance Computing (HPC) resources, follow the steps outlined below:</p>"},{"location":"connecting/getting-account/#step-1-send-a-registration-application","title":"Step 1: Send a Registration Application","text":""},{"location":"connecting/getting-account/#information-to-include-in-the-application","title":"Information to Include in the Application","text":"<p>In your registration application, make sure to include the following details:</p> <ul> <li> <p>Your Name</p> </li> <li> <p>Your Email Address</p> </li> <li> <p>The Cluster You Wish to Access (Specify either SMILE's Cluster or LULU's Cluster)</p> </li> <li> <p>Intended use of the HPC resources</p> </li> </ul>"},{"location":"connecting/getting-account/#contact-information-for-cluster-administrators","title":"Contact Information for Cluster Administrators","text":""},{"location":"connecting/getting-account/#smiles-cluster","title":"SMILE's Cluster","text":"<p>Administrator: Mr. Duy</p> <p>Email: khuongduying@gmail.com</p>"},{"location":"connecting/getting-account/#lulus-cluster","title":"LULU's Cluster","text":"<p>Administrator: Mr. Giang</p> <p>Email: nttg8100@gmail.com</p> <p>Send this email to the appropriate administrator for the cluster you wish to access, and they will provide you with your account.</p>"},{"location":"connecting/getting-account/#step-2-daily-usage-registration","title":"Step 2: Daily Usage Registration","text":""},{"location":"connecting/getting-account/#obtain-the-registration-form","title":"Obtain the Registration Form","text":"<p>You can obtain the registration form from the respective cluster administrator.</p>"},{"location":"connecting/getting-account/#complete-the-registration-form","title":"Complete the Registration Form","text":"<p>Submit the form to the administrator before 9:00 AM each day you intend to use the HPC resources. </p> <p>Provide all the required information accurately:</p> <ul> <li> <p>User: Your Name</p> </li> <li> <p>CPU: Number of CPUs requested</p> </li> <li> <p>Memory: Amount of memory requested</p> </li> <li> <p>Storage: Amount of storage requested</p> </li> <li> <p>Start Time: Expected start time of your job</p> </li> <li> <p>End Time: Expected end time of your job</p> </li> <li> <p>Expected Complete: Expected completion time of your job</p> </li> <li> <p>Job Description: Description of the job (required)</p> </li> <li> <p>Status: Current status of the job (if applicable)</p> </li> </ul> <p>By following these steps, you will be able to obtain an account and use the HPC resources provided by SMILE's and LULU's clusters effectively.</p>"},{"location":"connecting/ssh-connection/","title":"SSH Connection","text":"<p>SSH, or Secure Shell, lets you securely connect to remote computers (usually Linux/UNIX) over a network. </p> <p>Imagine you have a powerful computer cluster far away. SSH allows you to connect to those machines and control them from your own computer.</p> <p>Here's how to get started:</p>"},{"location":"connecting/ssh-connection/#1-installation","title":"1. Installation","text":"<p>Linux/UNIX: Install the <code>openssh-client</code> package.</p> <p>Windows: Consider tools like MobaXterm or Putty.</p>"},{"location":"connecting/ssh-connection/#2-connection","title":"2. Connection","text":"<p>Your local machine is the \"client\" and the remote one is the \"server.\" You'll need the server's hostname, IP address (maybe port number), username, and password.</p>"},{"location":"connecting/ssh-connection/#for-linuxunix","title":"For Linux/UNIX :","text":"<p>Open a terminal and type:</p> <pre><code># Default port\n$ ssh &lt;username&gt;@&lt;hostname-or-ip-address&gt;\n\n# Non-default port\n$ ssh &lt;username&gt;@&lt;hostname-or-ip-address&gt; -p &lt;port-number&gt;\n</code></pre>"},{"location":"connecting/ssh-connection/#for-windows-mobaxterm","title":"For Windows (MobaXterm):","text":"<ul> <li> <p>Open MobaXterm</p> </li> <li> <p>Create a new session (Sessions -&gt; New Session)</p> </li> <li> <p>Select the SSH icon and enter the cluster login node address (e.g., tcp.ap.ngrok.io for SMILE's Cluster or giangnguyen.zapto.org for LULU's Cluster)</p> </li> <li> <p>Specify your account_name as the username</p> </li> <li> <p>Under \"Advanced SSH Settings,\" enable \"Use private key\" and choose your private key file (<code>id_rsa.ppk</code>)</p> </li> <li> <p>Click OK</p> </li> <li> <p>Your saved session should appear in the MobaXterm main window</p> </li> </ul>"},{"location":"connecting/ssh-connection/#configure-ssh-client","title":"Configure SSH Client","text":"<p>For a more convenient connection to the cluster, create a personalized SSH configuration file. This reduces typing significantly. Add these lines to <code>~/.ssh/config</code>, replacing <code>USER_NAME</code> with your cluster username. Customize Host naming as preferred.</p> <pre><code>Host smile\n    HostName tcp.ap.ngrok.io\n    User USER_NAME\n\nHost lulu\n    HostName giangnguyen.zapto.org\n    User USER_NAME\n</code></pre> <p>After configuring this file, you can simply type the following command to connect without needing to recall the login node's hostname:</p> <pre><code>$ ssh smile\n</code></pre>"},{"location":"slurm/commands/","title":"Slurm commands","text":"Command Example Description my-accounts <code>$ my-accounts</code> Show your Slurm accounts. sacct <code>$ sacct</code> Show details about your recent jobs. sbatch <code>$ sbatch slurm_script.sh</code> Submit a batch job. scancel <code>$ scancel --me</code> (all jobs) Cancel all jobs <code>$ scancel &lt;jobID&gt;</code> (single job) Cancel a single job. scontrol <code>$ scontrol --help</code> (all options) Show all options for scontrol. <code>$ scontrol show job &lt;jobID&gt;</code> Show details of a specific job. sinfo <code>$ sinfo</code> (general) Show general information about the cluster. <code>$ sinfo -o '%11P %5D %22N %4c %21G %7m %11l'</code> Show detailed information about the cluster. sinteractive <code>$ sinteractive</code> (CPU only) Submit an interactive job (CPU only). <code>$ sinteractive --gres=gpu:&lt;numGPUs&gt;</code> Submit an interactive job with GPUs. squeue <code>$ squeue</code> (all jobs) Monitor all jobs. <code>$ squeue --me</code> (your jobs) Monitor your jobs. time-until-maintenance <code>$ time-until-maintenance</code> Show upcoming maintenance windows."},{"location":"slurm/job-scripts/","title":"Slurm Job Scripts","text":"<p>This page describes how to create SLURM job scripts.</p> <p>SLURM job scripts look as follows. On the top you have lines starting with <code>#SBATCH</code>. These appear as comments to bash scripts. These lines are interpreted by <code>sbatch</code> in the same way as command line arguments. That is, when later submitting the script with <code>sbatch my-job.sh</code> you can either have the parameter to the <code>sbatch</code> call or in the file.</p> <p>Multi-Node Allocation in Slurm</p> <p>Classically, jobs on HPC systems are written in a way that they can run on multiple nodes at once, using the network to communicate. Slurm comes from this world and when allocating more than one CPU/core, it might allocate them on different nodes. Please use <code>--nodes=1</code> to force Slurm to allocate them on a single node.</p> <p>Creating the Script</p> <pre><code>host:example$ cat &gt;my-job.sh &lt;&lt;\"EOF\"\n#!/bin/bash\n#\n#SBATCH --job-name=this-is-my-job\n#SBATCH --output=output.txt\n#\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=10:00\n#SBATCH --mem-per-cpu=100M\n\ndate\n\nhostname\n&gt;&amp;2 echo \"Hello World\"\n\nsleep 1m\n\ndate\nEOF\n</code></pre> <p>Submit, Look at Queue &amp; Result</p> <pre><code>host:example$ sbatch script.sh \nSubmitted batch job 315\nhost:example$ squeue  -u holtgrem_c\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n               315     debug this-is- holtgrem  R       0:40      1 med0127 \nhost:example$ sleep 2m\nhost:example$ squeue  -u holtgrem_c\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \nhost:example$ cat output.txt \nWed Mar 25 13:30:56 CET 2020\nmed0127\nHello World\nWed Mar 25 13:31:56 CET 2020\n</code></pre>"},{"location":"slurm/overview-slurm/","title":"Introduction to Scheduling","text":"<p>As explained elsewhere in more detail, an HPC cluster consists of multiple computers connected via a network and working together. Multiple users can use the system simultaneously to do their work. This means that the system needs to join multiple computers (nodes) to provide a coherent view of them and the same time partition the system to allow multiple users to work concurrently.</p> <pre><code>    user 1         user 2          ...\n\n  .---. .---.    .---. .---.\n  | J | | J |    | J | | J |\n  | o | | o |    | o | | o |       ...\n  | b | | b |    | b | | b |\n  | 1 | | 2 |    | 3 | | 4 |\n  '---' '---'    '---' '---'\n\n.------------------------------------------.\n|            Cluster Scheduler             |\n'------------------------------------------'\n\n.----------.  .------------.  .------------.\n| multiple |  |  separate  |  | computers  |\n'----------'  '------------'  '------------'\n</code></pre>"},{"location":"slurm/overview-slurm/#interlude-partitioning-single-computers","title":"Interlude: Partitioning Single Computers","text":"<p>Overall, this partitioning is not so different from how your workstation or laptop works. Most likely, your computer (or even your smartphone) has multiple processors (or cores). You can run multiple programs on the same computer and the fact that (a) there is more than one core and (b) there is more than one program running is not known to the running programs (unless they explicitly communicate with each other). Different programs can explicitly take advantage of the multiple processor cores. The main difference is that you normally use your computer in an interactive fashion (you perform an action and expect an immediate reaction).</p> <p>Even with a single processor (and core), your computer manages to run more than one program at the same time. This is done with the so-called time-slicing approach where the operating system lets each programs run in turn for a short time (a few milliseconds). A program with a higher priority will get more time slices than one with a lower (e.g., your audio player has real-time requirements and you will hear artifacts if it is starved for compute resources). Your operating system protects programs from each other by creating an address space for each. When two programs are running, the value of the memory at any given position in one program is independent from the value in the other program. Your operating system offers explicit functionality for sharing certain memory areas that two programs can use to exchange data efficiently.</p> <p>Similarly, file permissions with Unix users/groups or Unix/Windows ACLs (access control lists) are used to isolate users from each other. Programs can share data by accessing the same file if they can both access it. There are special files called sockets that allow for network-like inter-process communication but of course two programs on the same computer can also connect (virtually) via the computer network (no data will actually go through a cable).</p>"},{"location":"slurm/overview-slurm/#interlude-resource-types","title":"Interlude: Resource Types","text":"<p>As another diversion, let us consider how Unix manages its resources. This is important to understand when requesting resources from the scheduler later on.</p> <p>First of all, a computer might offer a certain feature such as a specific hardware platform or special network connection. Examples for this on the BIH HPC are specific Intel processor generations such as <code>haswell</code> or the availability of Infiniband networking. You can request these with so-called constraints; they are not allocated to specific jobs.</p> <p>Second, there are resources that are allocated to specific jobs. The most important resources here are:</p> <ul> <li>computing resources (processors/CPUs (central progressing units) and cores, details are explained below),</li> <li>main memory / RAM,</li> <li>special hardware such as GPUs, and</li> <li>(wall-clock) time that a job wants to run as an upper bound.</li> </ul> <p>Generally, once a resource has been allocated to one job, it is not available to another. This means if you allocating more resources to your job that you actually need (overallocation) then those resources are not available to other jobs (whether they are your jobs or those of other users). This will be explained further below.</p> <p>Another example of resource allocation are licenses. The BIH HPC has a few Matlab 2016b licenses that users can request. As long as a license is allocated to one job, it is unavailable to another.</p>"},{"location":"slurm/overview-slurm/#nodes-sockets-processors-cores-threads","title":"Nodes, Sockets, Processors, Cores, Threads","text":"<p>Regarding compute resources, Slurm differentiates between:</p> <ul> <li>nodes: a compute server,</li> <li>sockets: a socket in the compute server that hosts one physical processor,</li> <li>processor: a CPU or a CPU core in a multi-core computer (all CPUs in the BIH HPC are multi-core), and</li> <li>(hardware) threads: most Intel CPUs feature hardware threads (also known as \"hyperthreading\") where each core appears to be two cores.</li> </ul> <p>In most cases, you will use one compute node only. When using more than one node, you will need to use some form of message passing, e.g., MPI, so processes on different nodes can communicate. On a single node you would mostly use single- or multi-threaded processes, or multiple processes.</p> <p>Above: Slurm's nomenclature for sockets, processors, cores, and threads (from Slurm Documentation).</p> <p>Co-locating processes/threads on the same socket has certain implications that are mostly useful for numerical applications. We will not further go into detail here. Slurm provides many different features of ways to specify allocation of \"pinning\" to specific process locations. If you need this feature, we trust that you find sufficient explanation in the Slurm documentation.</p> <p>Usually, you would allocate multiple cores (a term Slurm uses synonymously with processors) on a single node (allocation on a single node is the default).</p>"},{"location":"slurm/overview-slurm/#how-scheduling-works","title":"How Scheduling Works","text":"<p>Slurm is an acronym for \"Simple Linux Unix Resource Manager\" (note that the word \"scheduler\" does not occur here). Actually, one classically differentiates between the managing of resources and the scheduling of jobs that use them. The resource manager allocates resources according to a user's request for a job and ensures that there are no conflicts. If the required resources are not available, the scheduler puts the user's job into a queue. Later, when then requested resources become available the scheduler assigns them to the job and runs it. In the following, both resource allocation and the running of the job are described as being done by the scheduler.</p> <p>The interesting case occurs when there are not enough resources available for at least two jobs submitted to the scheduler. The scheduler has to decide how to proceed. Consider the simplified case of only scheduling cores. Each job will request a number of cores. The scheduler will then generate a scheduling plan that might look as follows.</p> <pre><code>core\n  ^\n4 |   |---job2---|\n3 |   |---job2---|\n2 |   |---job2---|\n1 | |--job1--|\n  +--------------------------&gt; t time\n       5    1    1    2\n            0    5    0\n</code></pre> <p><code>job1</code> has been allocated one core and <code>job2</code> has been allocated two cores. When <code>job3</code>, requesting one core is submitted at t = 5, it has to wait at least as long until <code>job1</code> is finished. If <code>job3</code> requested two or more cores, it would have to wait at least until <code>job2</code> also finished.</p> <p>We can now ask several questions, including the following:</p> <ol> <li>What if a job runs for less than the allocated time?     -- In this case, resources become free and the scheduler will attempt to select the next job(s) to run.</li> <li>What if a job runs longer than the allocated time?     -- In this case, the scheduler will send an informative Unix signal to the process first.     The job will be given a bit more time and if it does not exit it will be forcibly terminated.     You will find a note about this at the end of your job log file.</li> <li>What if multiple jobs compete for resources?     -- The scheduler will prefer certain jobs over others using the Slurm Multifactor Priority Plugin.     In practice, small jobs will be preferred over large, users with few used resources in the last month will be favored over heavy consumers, long-waiting jobs will be favored over recently submitted jobs, and many other factors.     You can use the sprio utility to inspect these factors in real-time.</li> <li>How does the scheduler handle new requests?     -- Generally, the scheduler will add new jobs to the waiting queue.     The scheduler regularly adjusts its planning by recalculating job priorities.     Slurm is configured to perform computationally simple schedule recalculations quite often and larger recalculations more infrequently.</li> </ol> <p>Also see the Slurm Frequently Asked Questions.</p> <p>Please note that even if all jobs were known at the start of time, scheduling is still a so-called NP-complete problem. Entire computer science journals and books are dedicated only to scheduling. Things get more complex in the case of online scheduling, in which new jobs can appear at any time. In practice, Slurm does a fantastic job with its heuristics but it heavily relies on parameter tuning. HPC administration is constantly working on optimizing the scheduler settings. Note that you can use the <code>--format</code> option to the <code>squeue</code> command to request that it shows you information about job scheduling (in particular, see the <code>%S</code> field, which will show you the expected start time for a job, assuming Slurm has calculated it). See <code>man squeue</code> for details.</p>"},{"location":"slurm/overview-slurm/#slurm-partitions","title":"Slurm Partitions","text":"<p>In Slurm, the nodes of a cluster are split into partitions. Nodes are assigned to one or more partition section for details). Jobs can also be assigned to one or more partitions and are executed on nodes of the given partition.</p> <p>In the BIH HPC, partitions are used to stratify jobs of certain running times and to provide different quality of service (e.g., maximal number of CPU cores available to a user for jobs of a certain running time and size). The partitions <code>gpu</code> and <code>highmem</code> provide special hardware (the nodes are not assigned to other partitions) and the <code>mpi</code> partition allows MPI-parallelism and the allocation of jobs to more than one node.</p>"},{"location":"slurm/run-job/","title":"Slurm Quickstart","text":"<p>Create an interactive bash session (<code>srun</code> will run bash in real-time, <code>--pty</code> connects its <code>stdout</code> and <code>stderr</code> to your current session).</p> <pre><code>hpc-login-1:~$ srun --pty bash -i\nmed0740:~$ echo \"Hello World\"\nHello World\nmed0740:~$ exit\nhpc-login-1:~$\n</code></pre> <p>Note you probably want to longer running time for your interactive jobs. This way, your jobs can run for up to 28 days. This will make your job be routed automatically into the <code>long</code> partition as it is the only one that can fit your job.</p> <pre><code>hpc-login-1:~$ srun --pty --time 28-00 bash -i\nmed0740:~$\n</code></pre> <p>Pro-Tip: Using Bash aliases for quick access.</p> <pre><code>hpc-login-1:~$ alias slogin=\"srun --pty bash -i\"\nhpc-login-1:~$ slogin\nmed0740:~$ exit\nhpc-login-1:~$ cat &gt;&gt;~/.bashrc &lt;&lt;\"EOF\"\n# Useful aliases for logging in via Slurm\nalias slogin=\"srun --pty bash -i\"\nalias slogin-x11=\"srun --pty --x11 bash -i\"\nEOF\n</code></pre> <p>Create an interactive R session on the cluster (assuming conda is active and the environment <code>my-r</code> is created, e.g., with <code>conda create -n my-r r</code>).</p> <pre><code>hpc-login-1:~$ conda activate my-r\nhpc-login-1:~$ srun --pty R\nR version 3.6.2 (2019-12-12) -- \"Dark and Stormy Night\"\nCopyright (C) 2019 The R Foundation for Statistical Computing\n[...]\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n\n&gt; Sys.info()[\"nodename\"]\n nodename\n\"med0740\"\n&gt; q()\nSave workspace image? [y/n/c]:\nhpc-login-1:~$\n</code></pre> <p>Create an interactive iPython session on the cluster (assuming conda is active and the environment <code>my-python</code> is created, e.g., with <code>conda create -n my-python python=3 ipython</code>).</p> <pre><code>hpc-login-1:~$ conda activate my-python\nhpc-login-1:~$ srun --pty ipython\nPython 3.8.2 | packaged by conda-forge | (default, Mar  5 2020, 17:11:00)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.13.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import socket; socket.gethostname()\nOut[1]: 'med0740'\n\nIn [2]: exit\nhpc-login-1:~$\n</code></pre> <p>Allocate 4 cores (default is 1 core), and a total of 4GB of RAM on one node (alternatively use <code>--mem-per-cpu</code> to set RAM per CPU); <code>sbatch</code> accepts the same argument.</p> <pre><code>hpc-login-1:~$ srun --cpus-per-task=4 --nodes=1 --mem=4G --pty bash\nmed0740:~$ export | grep SLURM_CPUS_ON_NODE\n4\nmed0740:~$ your-parallel-script --threads 4\n</code></pre> <p>Submit an R script to the cluster in batch mode (<code>sbatch</code> schedules the job for later execution).</p> <pre><code>hpc-login-1:~$ cat &gt;job-script.sh &lt;&lt;\"EOF\"\n#!/bin/bash\necho \"Hello, I'm running on $(hostname) and it's $(date)\"\nEOF\nhpc-login-1:~$ sbatch job-script.sh\nSubmitted batch job 7\n\n# Some time later:\nhpc-login-1:~$ cat slurm-7.out\nHello, I'm running on med0740 and it's Fri Mar  6 07:36:42 CET 2020\nhpc-login-1:~$\n</code></pre>"},{"location":"slurm/snakemake/","title":"Snakemake with Slurm","text":"<p>This page describes how to use Snakemake with Slurm.</p>"},{"location":"slurm/snakemake/#prerequisites","title":"Prerequisites","text":"<ul> <li>This assumes that you have Miniconda properly setup with Bioconda.</li> <li>Also it assumes that you have already activated the Miniconda base environment with <code>source miniconda/bin/activate</code>.</li> </ul>"},{"location":"slurm/snakemake/#environment-setup","title":"Environment Setup","text":"<p>We first create a new environment <code>snakemake-slurm</code> and activate it. We need the <code>snakemake</code> package for this.</p> <pre><code>host:~$ conda create -y -n snakemake-slurm snakemake\n[...]\n#\n# To activate this environment, use\n#\n#     $ conda activate snakemake-slurm\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\nhost:~$ conda activate snakemake-slurm\n(snakemake-slurm) host:~$\n</code></pre>"},{"location":"slurm/snakemake/#snakemake-workflow-setup","title":"Snakemake Workflow Setup","text":"<p>We create a workflow and ensure that it works properly with multi-threaded Snakemake (no cluster submission here!)</p> <pre><code>host:~$ mkdir -p snake-slurm\nhost:~$ cd snake-slurm\nhost:snake-slurm$ cat &gt;Snakefile &lt;&lt;\"EOF\"\nrule default:\n    input: \"the-result.txt\"\n\nrule mkresult:\n    output: \"the-result.txt\"\n    shell: r\"sleep 1m; touch the-result.txt\"\nEOF\nhost:snake-slurm$ snakemake --cores=1\n[...]\nhost:snake-slurm$ ls\nSnakefile  the-result.txt\nhost:snake-slurm$ rm the-result.txt\n</code></pre>"},{"location":"slurm/snakemake/#snakemake-and-slurm","title":"Snakemake and  Slurm","text":"<p>You have two options:</p> <ol> <li>Simply use <code>snakemake --profile=cubi-v1</code> and the Snakemake resource configuration as shown below. STRONGLY PREFERRED</li> <li>Use the <code>snakemake --cluster='sbatch ...'</code> command.</li> </ol> <p>Note that we sneaked in a <code>sleep 1m</code>? In a second terminal session, we can see that the job has been submitted to SLURM indeed.</p> <pre><code>host:~$ squeue  -u holtgrem_c\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               325     debug snakejob holtgrem  R       0:47      1 med0127\n</code></pre>"},{"location":"slurm/snakemake/#threads-resources","title":"Threads &amp; Resources","text":"<p>The <code>cubi-v1</code> profile (stored in <code>/etc/xdg/snakemake/cubi-v1</code> on all cluster nodes) supports the following specification in your Snakemake rule:</p> <ul> <li><code>threads</code>: the number of threads to execute the job on</li> <li>memory in a syntax understood by Slurm, EITHER<ul> <li><code>resources.mem</code>/<code>resources.mem_mb</code>: the memory to allocate for the whole job, OR </li> <li><code>resources.mem_per_thread</code>: the memory to allocate for each thread.</li> </ul> </li> <li><code>resources.time</code>: the running time of the rule, in a syntax supported by Slurm, e.g. <code>HH:MM:SS</code> or <code>D-HH:MM:SS</code></li> <li><code>resources.partition</code>: the partition to submit your job into (Slurm will pick a fitting partition for you by default)</li> <li><code>resources.nodes</code>: the number of nodes to schedule your job on (defaults to <code>1</code> and you will want to keep that value unless you want to use MPI)</li> </ul> <p>You will need Snakemake &gt;=7.0.2 for this.</p> <p>Here is how to call Snakemake:</p> <pre><code># snakemake --profile=cubi-v1 -j1\n</code></pre> <p>To set rule-specific resources:</p> <pre><code>rule myrule:\n    threads: 1\n    resources:\n        mem='8G',\n        time='04:00:00',\n    input: # ...\n    output: # ...\n    shell: # ...\n</code></pre> <p>You can combine this with Snakemake resource callables, of course:</p> <pre><code>def myrule_mem(wildcards, attempt):\n    mem = 2 * attempt\n    return '%dG' % mem\n\nrule snps:\n    threads: 1\n    resources:\n        mem=myrule_mem,\n        time='04:00:00',\n    input: # ...\n    output: # ...\n    shell: # ...\n</code></pre>"},{"location":"slurm/snakemake/#custom-logging-directory","title":"Custom logging directory","text":"<p>By default, slurm will write log files into the working directory of snakemake, which will look like <code>slurm-$jobid.out</code>.</p> <p>To change this behaviour, the environment variable <code>SBATCH_DEFAULTS</code> can be set to re-route the <code>--output</code> parameter. If you want to write your files into <code>slurm_logs</code> with a filename pattern of <code>$name-$jobid</code> for instance, consider the following snippet for your submission script:</p> <pre><code>#!/bin/bash\n#\n#SBATCH --job-name=snakemake_main_job\n#SBATCH --ntasks=1\n#SBATCH --nodes=1\n#SBATCH --time=48:10:00\n#SBATCH --mem-per-cpu=300M\n#SBATCH --output=slurm_logs/%x-%j.log\n\nmkdir -p slurm_logs\nexport SBATCH_DEFAULTS=\" --output=slurm_logs/%x-%j.log\"\n\ndate\nsrun snakemake --use-conda -j1 --profile=cubi-v1\ndate\n</code></pre> <p>The name of the snakemake slurm job will be <code>snakemake_main_job</code>, the name of the jobs spawned from it will be called after the rule name in the Snakefile.</p>"}]}